"""
Test script for Ollama integration

Tests OllamaLLMCaller with local Ollama server.
"""

import asyncio
import contextlib
import logging
import sys
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(REPO_ROOT / "src"))

from dictator.voice.events import EventPubSub, EventType  # noqa: E402
from dictator.voice.llm_caller import OllamaLLMCaller  # noqa: E402


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger("OllamaTest")


async def test_ollama_basic():
    """Test basic Ollama interaction"""
    logger.info("=" * 60)
    logger.info("TEST 1: Basic Ollama Call")
    logger.info("=" * 60)

    # Create event pub/sub
    pubsub = EventPubSub()

    # Create Ollama caller
    ollama = OllamaLLMCaller(
        pubsub=pubsub,
        base_url="http://localhost:11434",
        model="llama3.2:latest"
    )

    # Process transcription
    test_input = "Ol√°, como voc√™ est√°?"
    logger.info(f"üì§ Input: {test_input}")

    # Start event listener
    async def listen_events():
        """Listen for events"""
        responses = []
        async for event in pubsub.poll():
            logger.info(f"üì¨ Event: {event.type}")

            if event.type == EventType.LLM_RESPONSE_COMPLETED:
                break
            elif event.type == EventType.TTS_SENTENCE_READY:
                responses.append(event.data['text'])
                logger.info(f"üí¨ Response: {event.data['text']}")
            elif event.type == EventType.LLM_RESPONSE_FAILED:
                logger.error(f"‚ùå Error: {event.data['error']}")
                break

        return responses

    # Run both tasks
    listener_task = asyncio.create_task(listen_events())
    await ollama.process_transcription(test_input)
    responses = await listener_task

    logger.info(f"‚úÖ Test completed: {len(responses)} response(s)")
    return responses


async def test_ollama_conversation():
    """Test conversation history"""
    logger.info("=" * 60)
    logger.info("TEST 2: Conversation History")
    logger.info("=" * 60)

    pubsub = EventPubSub()
    ollama = OllamaLLMCaller(
        pubsub=pubsub,
        base_url="http://localhost:11434",
        model="llama3.2:latest"
    )

    # Multi-turn conversation
    conversation = [
        "Meu nome √© Rodrigo",
        "Qual √© o meu nome?",
        "Em que linguagem estou falando?"
    ]

    for i, msg in enumerate(conversation, 1):
        logger.info(f"\n--- Turn {i} ---")
        logger.info(f"üì§ User: {msg}")

        # Listen for response
        async def listen_once():
            response = None
            async for event in pubsub.poll():
                if event.type == EventType.TTS_SENTENCE_READY:
                    response = event.data['text']
                    logger.info(f"üí¨ Assistant: {response}")
                elif event.type == EventType.LLM_RESPONSE_COMPLETED:
                    break
                elif event.type == EventType.LLM_RESPONSE_FAILED:
                    logger.error(f"‚ùå Error: {event.data['error']}")
                    break
            return response

        listener_task = asyncio.create_task(listen_once())
        await ollama.process_transcription(msg)
        await listener_task

    # Check history
    history = ollama.get_history()
    logger.info(f"\nüìö Conversation history: {len(history)} messages")
    for msg in history:
        logger.info(f"  {msg['role']}: {msg['content'][:50]}...")

    logger.info("‚úÖ Conversation test completed")


async def test_ollama_error_handling():
    """Test error handling with invalid server"""
    logger.info("=" * 60)
    logger.info("TEST 3: Error Handling")
    logger.info("=" * 60)

    pubsub = EventPubSub()
    ollama = OllamaLLMCaller(
        pubsub=pubsub,
        base_url="http://localhost:99999",  # Invalid port
        model="llama3.2:latest"
    )

    try:
        # Listen for error event
        async def listen_error():
            async for event in pubsub.poll():
                if event.type == EventType.LLM_RESPONSE_FAILED:
                    logger.info(f"‚úÖ Error event received: {event.data['error'][:100]}")
                    return True
                elif event.type == EventType.LLM_RESPONSE_COMPLETED:
                    logger.error("‚ùå Expected error but got success")
                    return False

        listener_task = asyncio.create_task(listen_error())
        await ollama.process_transcription("Test")
        result = await asyncio.wait_for(listener_task, timeout=5.0)

        if result:
            logger.info("‚úÖ Error handling test passed")

    except Exception as e:
        logger.info(f"‚úÖ Exception caught as expected: {type(e).__name__}")


async def test_provider_comparison():
    """Compare response quality between providers"""
    logger.info("=" * 60)
    logger.info("TEST 4: Provider Comparison")
    logger.info("=" * 60)

    test_prompt = "Explique em uma frase o que √© intelig√™ncia artificial"

    # Test Ollama
    logger.info("\nü¶ô Testing Ollama...")
    pubsub_ollama = EventPubSub()
    ollama = OllamaLLMCaller(
        pubsub=pubsub_ollama,
        base_url="http://localhost:11434",
        model="llama3.2:latest"
    )

    async def get_response(pubsub):
        async for event in pubsub.poll():
            if event.type == EventType.TTS_SENTENCE_READY:
                return event.data['text']
            elif event.type in (EventType.LLM_RESPONSE_COMPLETED, EventType.LLM_RESPONSE_FAILED):
                return None

    listener_ollama = asyncio.create_task(get_response(pubsub_ollama))
    await ollama.process_transcription(test_prompt)
    ollama_response = await listener_ollama

    logger.info(f"üì§ Prompt: {test_prompt}")
    logger.info(f"ü¶ô Ollama: {ollama_response}")

    # Compare characteristics
    logger.info("\nüìä Analysis:")
    logger.info(f"  Length: {len(ollama_response) if ollama_response else 0} chars")
    logger.info(f"  Has markdown: {'```' in ollama_response if ollama_response else False}")
    logger.info(f"  Sentences: {ollama_response.count('.') if ollama_response else 0}")

    logger.info("‚úÖ Comparison test completed")


async def test_tts_interrupt():
    """Test TTS interrupt when user speaks during TTS playback"""
    logger.info("=" * 60)
    logger.info("TEST 5: TTS Interrupt Simulation")
    logger.info("=" * 60)

    # Mock TTS engine
    class MockTTSEngine:
        def __init__(self):
            self.is_stopped = False
            self.stop_count = 0

        def stop(self):
            """Mock stop method"""
            self.stop_count += 1
            self.is_stopped = True
            logger.info("üõë MockTTSEngine.stop() called")

        def is_speaking(self):
            return not self.is_stopped

        def speak(self, text):
            """Mock speak method"""
            self.is_stopped = False
            logger.info(f"üîä MockTTSEngine speaking: {text[:50]}...")

    # Create components
    pubsub = EventPubSub()
    mock_tts = MockTTSEngine()

    # Import session manager to test interrupt logic
    from dictator.voice.session_manager import VoiceSessionManager  # noqa: E402

    # Mock callbacks
    def mock_stt(audio):
        return "Interrupt transcription"

    def mock_tts_callback(text):
        mock_tts.speak(text)

    # Create Ollama caller
    ollama = OllamaLLMCaller(
        pubsub=pubsub,
        base_url="http://localhost:11434",
        model="llama3.2:latest"
    )

    # Create session manager with mock TTS engine
    session = VoiceSessionManager(
        stt_callback=mock_stt,
        tts_callback=mock_tts_callback,
        llm_caller=ollama,
        vad_enabled=False,
        tts_engine=mock_tts  # Pass mock TTS engine
    )

    # Use the session's pubsub (VoiceSessionManager replaces the LLM caller's pubsub)
    pubsub = session.pubsub

    # Start session processor in the background (start() blocks until stop())
    session_task = asyncio.create_task(session.start())

    # Give event loop time to attach subscribers
    await asyncio.sleep(0.1)

    try:
        # Simulate TTS speaking by triggering actual TTS
        logger.info("üé§ Sending request to generate TTS response...")
        
        # Trigger LLM which will generate TTS
        async def monitor_and_interrupt():
            """Monitor for TTS start and interrupt it"""
            tts_started = False
            
            async for event in pubsub.poll():
                if event.type == EventType.TTS_SENTENCE_READY:
                    logger.info("üîä TTS started, waiting 0.5s then interrupting...")
                    tts_started = True
                    # Let TTS play for a bit
                    await asyncio.sleep(0.5)
                    
                    # Simulate user speaking by resetting flag
                    logger.info("üë§ User speaks - setting tts_speaking to False...")
                    session.tts_speaking = False
                    
                    # Wait to see if TTS stops
                    await asyncio.sleep(0.5)
                    break
                    
                elif event.type == EventType.LLM_RESPONSE_COMPLETED:
                    if not tts_started:
                        break
            
            return tts_started
        
        # Start monitoring task
        monitor_task = asyncio.create_task(monitor_and_interrupt())

        # Send simple request
        await ollama.process_transcription("Diga algo curto")

        # Wait for monitoring to complete
        tts_started = await asyncio.wait_for(monitor_task, timeout=15.0)
        
        # Give time for cleanup
        await asyncio.sleep(0.2)

        # Check results
        if tts_started:
            if mock_tts.stop_count > 0:
                logger.info(f"‚úÖ TTS interrupt triggered successfully! (stop called {mock_tts.stop_count} times)")
            else:
                logger.warning("‚ö†Ô∏è TTS was interrupted but stop() was not called (task cancelled)")
                logger.info("‚úÖ Interrupt mechanism working (flag-based cancellation)")
        else:
            logger.error("‚ùå TTS never started")

        # Check if flag was reset
        if not session.tts_speaking:
            logger.info("‚úÖ tts_speaking flag reset correctly")
        else:
            logger.error("‚ùå tts_speaking flag was not reset")

        logger.info("‚úÖ TTS interrupt test completed")

    finally:
        await session.stop()
        try:
            await asyncio.wait_for(session_task, timeout=2.0)
        except asyncio.TimeoutError:
            session_task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await session_task


async def main():
    """Run all tests"""
    logger.info("üöÄ Starting Ollama Integration Tests\n")

    try:
        # Test 1: Basic call
        await test_ollama_basic()
        await asyncio.sleep(1)

        # Test 2: Conversation
        await test_ollama_conversation()
        await asyncio.sleep(1)

        # Test 3: Error handling
        await test_ollama_error_handling()
        await asyncio.sleep(1)

        # Test 4: Provider comparison
        await test_provider_comparison()
        await asyncio.sleep(1)

        # Test 5: TTS interrupt
        await test_tts_interrupt()

        logger.info("\n" + "=" * 60)
        logger.info("üéâ All tests completed successfully!")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"\n‚ùå Test failed: {e}", exc_info=True)


if __name__ == "__main__":
    asyncio.run(main())
